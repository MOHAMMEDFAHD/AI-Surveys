{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📜 Chronological Evolution of RNNs\n",
        "\n",
        "---\n",
        "\n",
        "* **1901 | Santiago Ramón y Cajal**\n",
        "  * **Idea:** Observed recurrent semicircles in cerebellar cortex.  \n",
        "  * **Contribution:** First biological intuition of feedback loops in the brain.  \n",
        "  * **Gap Filled:** Highlighted natural recurrence as mechanism for memory.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1933 | Rafael Lorente de Nó**\n",
        "  * **Idea:** Discovered recurrent reciprocal connections in neurons.  \n",
        "  * **Contribution:** Proposed excitatory loops as basis of reflexes.  \n",
        "  * **Gap Filled:** Linked recurrence with dynamic brain behavior.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1943 | McCulloch & Pitts**\n",
        "  * **Paper:** *A Logical Calculus of Ideas Immanent in Nervous Activity*  \n",
        "  * **Idea:** Formal neuron model with recurrent (cyclic) connections.  \n",
        "  * **Contribution:** Showed networks with loops can depend on arbitrarily distant past activity.  \n",
        "  * **Gap Filled:** Theoretical foundation for RNN-like computation.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1960–1961 | Frank Rosenblatt**\n",
        "  * **Paper:** *Principles of Neurodynamics*  \n",
        "  * **Idea:** “Closed-loop cross-coupled perceptrons.”  \n",
        "  * **Contribution:** Early artificial recurrent perceptron networks.  \n",
        "  * **Gap Filled:** Linked Hebbian learning with recurrence.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1970s | Amari (1972), Little (1974)**\n",
        "  * **Idea:** Explored mathematical foundations of recurrent networks.  \n",
        "  * **Contribution:** Analyzed stability and learning dynamics.  \n",
        "  * **Gap Filled:** Built connection between RNNs and statistical mechanics.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1982 | John Hopfield**\n",
        "  * **Paper:** *Neural networks and physical systems with emergent collective computational abilities*  \n",
        "  * **Idea:** Hopfield network (recurrent with energy minimization).  \n",
        "  * **Contribution:** Introduced attractor dynamics, memory retrieval.  \n",
        "  * **Gap Filled:** Formalized associative memory using recurrence.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1986 | Jordan Network (Michael Jordan)**\n",
        "  * **Idea:** Context units fed from output back into hidden state.  \n",
        "  * **Contribution:** Early simple recurrent network (SRN).  \n",
        "  * **Gap Filled:** Added feedback for sequence modeling.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1990 | Elman Network (Jeffrey Elman)**\n",
        "  * **Idea:** Context units from hidden state feedback.  \n",
        "  * **Contribution:** Popular SRN for sequence prediction and language.  \n",
        "  * **Gap Filled:** Cognitive modeling of temporal sequences.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1993 | Jürgen Schmidhuber**\n",
        "  * **Paper:** *A Neural History Compressor*  \n",
        "  * **Idea:** Hierarchical RNN that compresses history.  \n",
        "  * **Contribution:** Tackled “very deep learning” with >1000 steps.  \n",
        "  * **Gap Filled:** Early attempt to manage long-term dependencies.  \n",
        "\n",
        "---\n",
        "\n",
        "* **1997 | Hochreiter & Schmidhuber**\n",
        "  * **Paper:** *Long Short-Term Memory*  \n",
        "  * **Idea:** Introduced LSTM with input/forget/output gates.  \n",
        "  * **Contribution:** Solved vanishing gradient, captured long-term dependencies.  \n",
        "  * **Gap Filled:** First practical RNN for long sequences.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2000 | Bidirectional RNN (Schuster & Paliwal)**\n",
        "  * **Idea:** Processing input both forward and backward.  \n",
        "  * **Contribution:** Improved context awareness.  \n",
        "  * **Gap Filled:** Enabled better sequence labeling tasks (e.g., speech).  \n",
        "\n",
        "---\n",
        "\n",
        "* **2006–2012 | Revival with Deep Learning**\n",
        "  * **Applications:** LSTM + BRNN used for speech recognition (Graves et al.).  \n",
        "  * **Contribution:** Outperformed HMM-based models.  \n",
        "  * **Gap Filled:** RNNs became state-of-the-art in ASR, handwriting recognition.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2014 | Cho et al.**\n",
        "  * **Paper:** *Learning Phrase Representations using RNN Encoder–Decoder*  \n",
        "  * **Idea:** Seq2Seq with RNN encoder–decoder.  \n",
        "  * **Contribution:** First full end-to-end NMT pipeline.  \n",
        "  * **Gap Filled:** Enabled neural machine translation.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2014 | Sutskever, Vinyals & Le**\n",
        "  * **Paper:** *Sequence to Sequence Learning with Neural Networks*  \n",
        "  * **Idea:** Large-scale Seq2Seq with LSTMs.  \n",
        "  * **Contribution:** Showed neural MT outperforming phrase-based MT.  \n",
        "  * **Gap Filled:** Validated deep RNNs for translation.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2014 | Cho et al. (GRU)**\n",
        "  * **Paper:** *Gated Recurrent Unit*  \n",
        "  * **Idea:** Simplified LSTM with update + reset gates.  \n",
        "  * **Contribution:** Computationally cheaper with similar performance.  \n",
        "  * **Gap Filled:** Efficient alternative to LSTM.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2015–2016 | RNN with Attention (Bahdanau, Luong, Cheng)**\n",
        "  * **Idea:** Cross-attention + self-attention on top of RNNs.  \n",
        "  * **Contribution:** Removed fixed bottleneck, improved MT and reading.  \n",
        "  * **Gap Filled:** Extended RNN usefulness but revealed scalability issues.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2017 | Vaswani et al.**\n",
        "  * **Paper:** *Attention is All You Need*  \n",
        "  * **Idea:** Transformer, removing recurrence.  \n",
        "  * **Contribution:** Outperformed RNNs across NLP tasks.  \n",
        "  * **Gap Filled:** Addressed RNN inefficiency + long dependencies.  \n",
        "\n",
        "---\n",
        "\n",
        "* **2020+ | Decline of RNN dominance**\n",
        "  * **Vision Transformers, GPT, BERT, etc.** replaced RNNs in NLP and CV.  \n",
        "  * **Contribution:** Attention-based models scaled better.  \n",
        "  * **Gap Filled:** RNNs pushed aside except in lightweight/real-time tasks.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Takeaway\n",
        "\n",
        "- **1901–1940s:** Neuroscience origins.  \n",
        "- **1960s–1980s:** Theoretical neural feedback.  \n",
        "- **1986–1990:** Early recurrent models (Jordan/Elman).  \n",
        "- **1997:** LSTM breakthrough.  \n",
        "- **2014:** Seq2Seq & GRU.  \n",
        "- **2015–2016:** RNN + Attention.  \n",
        "- **2017+:** Superseded by Transformers.  \n"
      ],
      "metadata": {
        "id": "lL_ARUbAsfoa"
      }
    }
  ]
}